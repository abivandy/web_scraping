{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Digital Trace Data: Web Scraping / APIs\n",
    "June 19th, 2018 - Javier Garcia-Bernardo & Allie Morgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Requirements\n",
    "import requests               # Simple HTTP operations (GET and POST)\n",
    "import selenium               # Loads dynamic (javascript) pages\n",
    "import json                   # Parsing the responses from APIs\n",
    "import re                     # Python library for parsing regular expressions\n",
    "from bs4 import BeautifulSoup # Parsing HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Web scraping](https://en.wikipedia.org/wiki/Web_scraping) is a method for extracting data from the web. There are many techniques which can be used for web scraping — ranging from requiring human involvement (“human copy-paste”) to fully automated systems (using computer vision). Somewhere in the middle is HTML parsing, which we will describe here.\n",
    "\n",
    "Web scraping using [HTML parsing](https://en.wikipedia.org/wiki/Web_scraping#HTML_parsing) is often used on webpages which share similar HTML structure. For example, you might want to scrape the ingredients from chocolate chip cookie recipes to identify correlations between ingredients and five-star worthy cookies, or you might want to predict who will win March Madness by looking at game play-by-plays, or you want to know all the local pets up for adoption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<head>\n",
      "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\" />\n",
      "<meta charset=\"utf-8\" />\n",
      "<link rel=\"shortcut icon\" href=\"https://www.boulderhumane.org/sites/default/files/favicon.ico\" type=\"image/vnd.microsoft.icon\" />\n",
      "<meta name=\"Generator\" content=\"Drupal 7 (http://drupal.org)\" />\n",
      "<meta name=\"viewport\" content=\"width=1000px, initial-scale=1.0, maximum-scale=1.0\" />\n",
      "<title>Dogs Available for Adoption | Humane Society of Boulder Valley</title>\n",
      "<link type=\"text/css\" rel=\"stylesheet\n"
     ]
    }
   ],
   "source": [
    "pet_pages = [\"https://www.boulderhumane.org/animals/adoption/dogs\", \n",
    "             \"https://www.boulderhumane.org/animals/adoption/cats\", \n",
    "             \"https://www.boulderhumane.org/animals/adoption/adopt_other\"]\n",
    "\n",
    "r = requests.get(pet_pages[0])\n",
    "html = r.text\n",
    "print(html[:500]) # Print the first 500 characters of the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you visit a webpage, your web browser renders an HTML document with CSS and Javascript to produce a visually appealing page. (See the HTML above.) [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) is a Python library for parsing HTML. We'll use it to extract all of the names, ages, and breeds of the [dogs](https://www.boulderhumane.org/animals/adoption/dogs), [cats](https://www.boulderhumane.org/animals/adoption/cats), and [small animals](https://www.boulderhumane.org/animals/adoption/adopt_other) currently up for adoption at the Boulder Humane Society."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the feature of these pages which we are exploiting is their repeated HTML structure. Every animal listed has the following HTML variant:\n",
    "```{html}\n",
    "<div class=\"views-row ... \">\n",
    "  ...\n",
    "  <div class=\"views-field views-field-field-pp-animalname\">\n",
    "    <div class=\"field-content\">\n",
    "      <a href=\"/animals/adoption/\" title=\"Adopt Me!\">Romeo</a>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-primarybreed\">\n",
    "    <div class=\"field-content\">New Zealand</div>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-secondarybreed\">\n",
    "    <div class=\"field-content\">Rabbit</div>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-age\">\n",
    "    ...\n",
    "    <span class=\"field-content\">0 years 2 months</span>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-gender\">\n",
    "    ...\n",
    "    <span class=\"field-content\">Male</span>\n",
    "  </div>\n",
    "  ...\n",
    "</div>\n",
    "``` \n",
    "So to get at the HTML object for each pet, we can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pets = soup.find_all('div', {'class': re.compile('.*views-row.*')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, find all of the `div` tags with the `class` attribute which contains the string `views-row`. \n",
    "\n",
    "Next to grab the name, breeds, and ages of these pets, we’ll grab the children of each pet HTML object. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Audi', u'Belgian Malinois', u'', u'Age:5 years 7 months')\n",
      "(u'Kobe', u'Cane Corso', u'', u'Age:1 year 8 months')\n",
      "(u'Roxy', u'Terrier, American Pit Bull', u'Mix', u'Age:1 year 6 months')\n",
      "(u'Bear', u'Retriever, Chesapeake Bay', u'Mix', u'Age:2 years 5 months')\n",
      "(u'Teddy', u'Terrier, Jack Russell', u'Mix', u'Age:6 years 0 months')\n",
      "(u'Chloe', u'Akita', u'Mix', u'Age:5 years 0 months')\n",
      "(u'Drew', u'Retriever, Labrador', u'Retriever, Golden', u'Age:2 years 0 months')\n",
      "(u'Harley', u'Terrier, American Pit Bull', u'Mix', u'Age:2 years 0 months')\n",
      "(u'Sadie', u'Great Dane', u'Retriever, Labrador', u'Age:8 years 10 months')\n",
      "(u'Megrita', u'Dutch Shepherd', u'Mix', u'Age:5 years 0 months')\n",
      "(u'Molly', u'Mastiff', u'Rottweiler', u'Age:8 years 0 months')\n",
      "(u'Butterbean', u'Terrier, Jack Russell', u'Mix', u'Age:7 years 0 months')\n",
      "(u'Rowdy', u'Rottweiler', u'Mix', u'Age:6 years 0 months')\n",
      "(u'Marco', u'Spaniel, American Cocker', u'', u'Age:5 years 0 months')\n",
      "(u'Eva', u'Boxer', u'Mix', u'Age:6 years 0 months')\n",
      "(u'Stew', u'Chihuahua, Short Coat', u'Mix', u'Age:7 years 0 months')\n",
      "(u'Sandlin', u'German Shepherd', u'Mix', u'Age:0 years 5 months')\n",
      "(u'Gabby', u'Akita', u'Mix', u'Age:0 years 10 months')\n",
      "(u'Tom', u'Beagle', u'Mix', u'Age:10 years 0 months')\n",
      "(u'Jenny', u'Coonhound, Black and Tan', u'', u'Age:6 years 3 months')\n",
      "(u'Shadow', u'German Shepherd', u'Mix', u'Age:1 year 3 months')\n",
      "(u'Dolly', u'Chihuahua, Short Coat', u'Mix', u'Age:11 years 0 months')\n",
      "(u'Sandy May', u'Coonhound', u'Mix', u'Age:7 years 0 months')\n",
      "(u'Spoodles', u'Chihuahua, Long Coat', u'Poodle, Miniature', u'Age:5 years 6 months')\n",
      "(u'Bruno', u'Vizsla, Smooth Haired', u'Beagle', u'Age:1 year 6 months')\n",
      "(u'Fen', u'Pointer', u'Mix', u'Age:1 year 7 months')\n",
      "(u'Elijah', u'Terrier, American Pit Bull', u'Mix', u'Age:0 years 8 months')\n",
      "(u'Madre', u'Terrier, American Pit Bull', u'Mix', u'Age:3 years 1 month')\n",
      "(u'Lulu', u'Boxer', u'Mix', u'Age:4 years 6 months')\n",
      "(u'Baloo', u'Beagle', u'Mix', u'Age:0 years 8 months')\n",
      "(u'Rocky', u'Retriever, Labrador', u'Mix', u'Age:3 years 1 month')\n",
      "(u'Bailey', u'Schnauzer, Miniature', u'Mix', u'Age:3 years 1 month')\n",
      "(u'Napoleon', u'Boxer', u'Mix', u'Age:0 years 6 months')\n"
     ]
    }
   ],
   "source": [
    "head = \"views-field views-field-field-pp-\"\n",
    "for pet in pets:\n",
    "    name = pet.find('div', {'class': head + 'animalname'}).get_text(strip=True)\n",
    "    primary_breed = pet.find('div', {'class': head + 'primarybreed'}).get_text(strip=True)\n",
    "    secondary_breed = pet.find('div', {'class': head + 'secondarybreed'}).get_text(strip=True)\n",
    "    age = pet.find('div', {'class': head + 'age'}).get_text(strip=True)\n",
    "    print(name, primary_breed, secondary_breed, age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where each call to `find` is getting the children of a pet object, in particular, the `div`s with `class` attributes which look like `views-field views-field-field-pp-*`. Feel free to replace the above code with the cat or small animal pages provided and see how the output changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic (Javascript) Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the webpages we were loading required no [Javascript](https://en.wikipedia.org/wiki/JavaScript). In other words, there was no input required on the users end to view the content of the page (e.g. a login). Let's try a more complicated example of webscraping where content is loaded dynamically.\n",
    "\n",
    "\n",
    "Some characterirstics:\n",
    "- It's slow\n",
    "- It can handle javascript\n",
    "- You get **html** code back\n",
    "- Behave like a person\n",
    "\n",
    "\n",
    "Requirements (one):\n",
    "- Firefox + geckodriver (https://github.com/mozilla/geckodriver/releases)\n",
    "- Chrome + chromedriver (https://sites.google.com/a/chromium.org/chromedriver/)\n",
    "\n",
    "    \n",
    "geckodriver/chromedriver must have execution permissions (chmod +x geckodriver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define two functions to wait until the page has finished loading. Most times this is not needed but it doesn't hurt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import selenium.webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the browser and define how much are you willing to wait for a page to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#open the driver (change the executable path to geckodriver_mac or geckodriver.exe)\n",
    "driver = selenium.webdriver.Chrome(executable_path=\"./chromedriver\")\n",
    "driver.implicitly_wait(10)\n",
    "driver.set_page_load_timeout(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get xkcd and click through the comics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a website\n",
    "driver.get(\"https://xkcd.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's find the random buttom\n",
    "element = driver.find_element_by_xpath('//*[@id=\"middleContainer\"]/ul[1]/li[3]/a')\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find an attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The problem with Haskell is that it's a language built on lazy evaluation and nobody's actually called for it.\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element = driver.find_element_by_xpath('//*[@id=\"comic\"]/img')\n",
    "element.get_attribute(\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log in in spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##DO NOT WRITE YOUR PASSWORD IN NOTEBOOKS!!\n",
    "fb_email, fb_pass = \"f1692418@mvrht.com\",\"pedropalotes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#go to facebook\n",
    "driver.get(\"https://www.facebook.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#send email and password\n",
    "driver.find_element_by_xpath('//*[@id=\"email\"]').send_keys(fb_email)\n",
    "driver.find_element_by_xpath('//*[@id=\"pass\"]').send_keys(fb_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#click on login\n",
    "driver.find_element_by_xpath('//*[@id=\"loginbutton\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find JP\n",
    "element = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div/div[1]/div/div/div/div[1]/div[2]/div/form/div/div/div/div/input[2]')\n",
    "element.send_keys(\"john paul gonzales\")\n",
    "element = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div/div[1]/div/div/div/div[1]/div[2]/div/form/button')\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#click on him\n",
    "element = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[1]/div/div[3]/div[2]/div/div/div[3]/div/div/div/div[1]/div/div/div/div/div/div[2]/div[1]/div/div[1]/a/div')\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#send a friend request\n",
    "element = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[1]/div/div[2]/div[2]/div[2]/div/div[1]/div/div[4]/div/div[2]/div/div[2]/span/span/span[1]/a')\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow users to access large amounts of data, companies may provide an [Application Programming Interface (API)](https://en.wikipedia.org/wiki/Application_programming_interface). Often these request are handled via PUT and POST HTTP requests. For example, to make a request from the Twitter API:\n",
    "\n",
    "```{bash}\n",
    "curl --request GET \n",
    " --url 'https://api.twitter.com/1.1/search/tweets.json?q=nasa&result_type=popular' \n",
    " --header 'authorization: OAuth oauth_consumer_key=\"consumer-key-for-app\", ... , \n",
    " oauth_token=\"access-token-for-authed-user\", oauth_version=\"1.0\"'\n",
    " ```\n",
    "\n",
    "APIs often return data in the format of [Javascript Object Notation (JSON)](https://en.wikipedia.org/wiki/JSON). For example:\n",
    "\n",
    "```{json}\n",
    "{\"status\": 200, \"message\": \"hello world\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Hidden\" APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_names(letters):\n",
    "    params = (\n",
    "        ('name', letters),\n",
    "        ('request_num', '1'),\n",
    "    )\n",
    "\n",
    "    response = requests.get('https://www.uvm.edu/directory/api/query_results.php', params=params)\n",
    "    if response.ok == True:\n",
    "        return response.text\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status\":\"1\",\"data\":[{\"uvmedubroadcastflag\":{\"count\":\"1\",\"0\":\"a\"},\"0\":\"uvmedubroadcastflag\",\"uvmeduuuid\":{\"count\":\"1\",\"0\":\"fbc0019c-c8a1-11da-96a6-d0e9f2a749f2\"},\"1\":\"uvmeduuuid\",\"edupersonaffiliation\":{\"count\":\"2\",\"0\":\"Alum\",\"1\":\"Affiliate\"},\"2\":\"edupersonaffiliation\",\"uvmeduaffiliation\":{\"count\":\"2\",\"0\":\"Former Student\",\"1\":\"Affiliated Organization Employee\"},\"3\":\"uvmeduaffiliation\",\"sn\":{\"count\":\"1\",\"0\":\"Sender\"},\"4\":\"sn\",\"uvmedusurname\":{\"count\":\"1\",\"0\":\"Sender\"},\"5\":\"uvmedusurname\",\"uvmaltuid\":{\"count\":\"1\",\"0\":\"502\"},\"6\":\"uvmaltuid\",\"uvmaltgid\":{\"count\":\"1\",\"0\":\"502\"},\"7\":\"uvmaltgid\",\"uvmalthomedir\":{\"count\":\"1\",\"0\":\"\\/Users\\/Customer\"},\"8\":\"uvmalthomedir\",\"edupersonentitlement\":{\"count\":\"7\",\"0\":\"urn:mace:uvm.edu:pgp:encryption-comis\",\"1\":\"urn:mace:uvm.edu:ets:unix-login\",\"2\":\"urn:mace:uvm.edu:ets:software-download\",\"3\":\"urn:mace:uvm.edu:ets:unix-filespace\",\"4\":\"urn:mace:uvm.edu:ets:email\",\"5\":\"urn:mace:uvm.edu:ets:ad-login\",\"6\":\"urn:mace:uvm.edu:ets:banner-access\"},\"9\":\"eduperso\n"
     ]
    }
   ],
   "source": [
    "response = get_names(\"ab\")\n",
    "print(response[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response_json = json.loads(response)\n",
    "response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staff adion1@uvm.edu Aaron Dion\n",
      "Faculty akindsva@uvm.edu Aaron Kindsvatter\n",
      "Student 04afogg@uvm.edu Aaron James Fogg\n",
      "Faculty areiter@uvm.edu Aaron W Reiter\n",
      "Faculty anichol3@uvm.edu Aaron F Nichols\n",
      "Student aricha15@uvm.edu Aaron Fields Richards\n",
      "Student aborkow1@uvm.edu Aaron Carl Borkowski\n",
      "Employee agreens3@uvm.edu Aaron S Greenspun\n",
      "Student alewy@uvm.edu Aaron J Lewy\n",
      "Student agelinne@uvm.edu Aaron Michael Gelinne\n"
     ]
    }
   ],
   "source": [
    "for i,person in enumerate(x[\"data\"]):\n",
    "    if i == 10: \n",
    "        break #make sure we don't print too much\n",
    "        \n",
    "        \n",
    "    print(person[\"edupersonprimaryaffiliation\"][\"0\"],\n",
    "    person[\"edupersonprincipalname\"][\"0\"],\n",
    "    person[\"cn\"][\"0\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit APIs\n",
    "\n",
    "1.1 Using the API that the site provides (e.g. Twitter).\n",
    "- You send a request and the site gives you back json (a dictionary with the data).\n",
    "- Chances are that somebody else have coded a python package for you.\n",
    "- They have limits.\n",
    "\n",
    "Example: Twitter\n",
    "- Get a key: https://apps.twitter.com/\n",
    "- Documentation: https://dev.twitter.com/rest/public\n",
    "- Find a library: https://dev.twitter.com/resources/twitter-libraries (let's use https://github.com/tweepy/tweepy)\n",
    "\n",
    "Limitations\n",
    "- 100 mess/query\n",
    "- 180 messages every 15 min\n",
    "- Only last seven days of data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________\n",
      "sfiscience None 26157\n",
      "Seats are sold out, but you can catch the livestream on our YouTube channel: https://t.co/Sf8LLAYOxk https://t.co/6O2gFDc03D\n",
      "__________\n",
      "sfiscience None 26157\n",
      "We're hiring a full-time social media specialist. This is one of the most dynamic positions @sfiscience. https://t.co/MBgWWTjUNC\n",
      "__________\n",
      "sfiscience None 26157\n",
      "Happening now at SFI: workshop brings together thinkers in disciplines ranging from cosmology to chronobiology to n… https://t.co/NCt5Fdtf2e\n",
      "__________\n",
      "sfiscience None 26157\n",
      "@NigelGoldenfeld Please present this next time you come to @sfiscience!\n",
      "__________\n",
      "sfiscience None 26157\n",
      "RT @NigelGoldenfeld: Finally out after 8 years of work!  Experimental observation of stochastic Turing patterns, in a forward-engineered bi…\n",
      "__________\n",
      "sfiscience None 26157\n",
      "RT @brianjdermody: Fantastic workshop on the complexities of human-water systems @sfiscience. Sooo many new ideas, gonna take some time to…\n",
      "__________\n",
      "sfiscience None 26157\n",
      "Congratulations indeed! https://t.co/yccaxYOMJk\n",
      "__________\n",
      "sfiscience None 26157\n",
      "Catch last Tuesday's interview about emerging media arts on KSFR  https://t.co/Klo1YXnH9q\n",
      "__________\n",
      "sfiscience None 26157\n",
      "Santa Fe's EMA launch party is tonight! The Emerging Media Alliance \"celebrates a new era of teamwork and achieveme… https://t.co/U1AsztM5fe\n",
      "__________\n",
      "sfiscience None 26157\n",
      "Genes of modern males suggest a mass die-off of men 5-7 thousand years ago. An analysis by External Professor Marc… https://t.co/0dWENIOGEy\n",
      "__________\n",
      "sfiscience None 26157\n",
      "The story features seminal research by SFI's Geoffrey West, Brian Enquist, and James Brown, published in Science in… https://t.co/jsRWoiu7Op\n",
      "__________\n",
      "sfiscience None 26157\n",
      "Neat story by @vcallier in @QuantaMagazine on the biophysics underlying the powerful jumps, punches, and bites in s… https://t.co/NwG9MRMSIu\n",
      "__________\n",
      "sfiscience None 26157\n",
      "Axle Contemporary, a mobile exhibit space based in Santa Fe, premiered a new experiential piece during… https://t.co/uKX0A8a6MA\n",
      "__________\n",
      "sfiscience None 26157\n",
      "Happening now at SFI: former SFI postdocs @MarionDumas1 and Christa Brelsford lead the Socio-Hydrological Dynamics… https://t.co/4GcOcXx2aG\n",
      "__________\n",
      "sfiscience None 26157\n",
      "@mcmsc01 @INSPI_ECUADOR @ASU @ASUBeingHuman @inguniandes @UNColombia @rhidalgott @VarsoviaC @DanLarremore\n",
      "__________\n",
      "sfiscience None 26157\n",
      "The kanji for \"beauty\" and address of the Bitcoin genesis block, drawn by @TuurDemeester, appear below the Gell-Mann equation.\n",
      "__________\n",
      "sfiscience None 26157\n",
      "The M Alpha Monolith now stands on SFI's Hyde Park campus. Etched in the stone, the glyph for \"M\" + Greek alpha ack… https://t.co/1uMUPfVRzG\n",
      "__________\n",
      "sfiscience None 26157\n",
      "The InterPlanetary Festival was the first event in a series that offers \"a new way of seeing.\"  @ABQJournal reports… https://t.co/csJToIgcWh\n",
      "__________\n",
      "sfiscience None 26157\n",
      "@julioccneto @DavidFeldman They are not, but you can take Liz Bradley's course in Nonlinear Dynamics through… https://t.co/KcUbXOvtrV\n",
      "__________\n",
      "sfiscience None 26157\n",
      "Come back anytime! https://t.co/PRJg7rDfEV\n",
      "__________\n",
      "sfiscience None 26157\n",
      "RT @TuurDemeester: After attending the inimitable  @IP_Fest, I was honored to be with @B3_MillerValue for the inauguration of the M Alpha M…\n",
      "__________\n",
      "sfiscience None 26157\n",
      "@mcmsc01 @MantillaIgnacio @michaelcrow @stevenstrogatz @cparedesverduga @rhidalgott @cmontufar @esantos1957… https://t.co/uUt7Cgwt9v\n",
      "Number of results: 22 (22 new)\n",
      "Number of results: 22 (0 new)\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "def twitter(d_keys,query):\n",
    "    #Authtentify\n",
    "    auth = tweepy.OAuthHandler(d_keys[\"CONSUMER_KEY\"], d_keys[\"CONSUMER_SECRET\"])\n",
    "    auth.set_access_token(d_keys[\"ACCESS_KEY\"], d_keys[\"ACCESS_SECRET\"])\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    #We want 1000 tweets\n",
    "    num_results = 1000\n",
    "    result_count = 0\n",
    "    last_id = None\n",
    "    \n",
    "    #Max 180 tweets 15 min\n",
    "    cumulative = 0\n",
    "\n",
    "    #While we don't have them\n",
    "    while (result_count <  num_results):\n",
    "        previous_tweets = result_count\n",
    "        #Ask for more tweets, starting in the last_id (identifier of the tweet)\n",
    "        results = api.search(q = query,\n",
    "                              count = 90, max_id = last_id,result_type=\"recent\")\n",
    "                                # geocode = \"{},{},{}km\".format(latitude, longitude, max_range) #for geocode\n",
    "\n",
    "        #for each tweet extract some info (json structure)\n",
    "        for result in results:\n",
    "            result_count += 1\n",
    "            user = result.user.screen_name\n",
    "            text = result.text\n",
    "            followers_count = result.user.followers_count\n",
    "            time_zone = result.user.time_zone\n",
    "            print(\"_\"*10)\n",
    "            print(user,time_zone,followers_count)\n",
    "            print(text)\n",
    "\n",
    "        #keep the last_id to know where to continue\n",
    "        last_id = int(result.id)-1\n",
    "        new_tweets = result_count - previous_tweets\n",
    "\n",
    "        print (\"Number of results: {} ({} new)\".format(result_count,new_tweets))\n",
    "\n",
    "        #If we don't get new tweets exit\n",
    "        if new_tweets == 0: \n",
    "            break\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        if ((result_count + 90) // 150) > cumulative:\n",
    "            cumulative += 1\n",
    "            time.sleep(15*60)\n",
    "\n",
    "\n",
    "d_keys = pickle.load(open(\".key\",\"rb\")) #don't share your keys ;)\n",
    "twitter(d_keys,\"from:sfiscience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
