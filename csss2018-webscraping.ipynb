{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Digital Trace Data: Web Scraping / APIs\n",
    "June 19th, 2018 - Javier Garcia-Bernardo & Allie Morgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Requirements\n",
    "import requests               # Simple HTTP operations (GET and POST)\n",
    "import selenium               # Loads dynamic (javascript) pages\n",
    "import json                   # Parsing the responses from APIs\n",
    "import re                     # Python library for parsing regular expressions\n",
    "from bs4 import BeautifulSoup # Parsing HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Web scraping](https://en.wikipedia.org/wiki/Web_scraping) is a method for extracting data from the web. There are many techniques which can be used for web scraping — ranging from requiring human involvement (“human copy-paste”) to fully automated systems (using computer vision). Somewhere in the middle is HTML parsing, which we will describe here.\n",
    "\n",
    "Web scraping using [HTML parsing](https://en.wikipedia.org/wiki/Web_scraping#HTML_parsing) is often used on webpages which share similar HTML structure. For example, you might want to scrape the ingredients from chocolate chip cookie recipes to identify correlations between ingredients and five-star worthy cookies, or you might want to predict who will win March Madness by looking at game play-by-plays, or you want to know all the local pets up for adoption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<head>\n",
      "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\" />\n",
      "<meta charset=\"utf-8\" />\n",
      "<link rel=\"shortcut icon\" href=\"https://www.boulderhumane.org/sites/default/files/favicon.ico\" type=\"image/vnd.microsoft.icon\" />\n",
      "<meta name=\"Generator\" content=\"Drupal 7 (http://drupal.org)\" />\n",
      "<meta name=\"viewport\" content=\"width=1000px, initial-scale=1.0, maximum-scale=1.0\" />\n",
      "<title>Dogs Available for Adoption | Humane Society of Boulder Valley</title>\n",
      "<link type=\"text/css\" rel=\"stylesheet\n"
     ]
    }
   ],
   "source": [
    "pet_pages = [\"https://www.boulderhumane.org/animals/adoption/dogs\", \n",
    "             \"https://www.boulderhumane.org/animals/adoption/cats\", \n",
    "             \"https://www.boulderhumane.org/animals/adoption/adopt_other\"]\n",
    "\n",
    "r = requests.get(pet_pages[0])\n",
    "html = r.text\n",
    "print(html[:500]) # Print the first 500 characters of the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you visit a webpage, your web browser renders an HTML document with CSS and Javascript to produce a visually appealing page. (See the HTML above.) [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) is a Python library for parsing HTML. We'll use it to extract all of the names, ages, and breeds of the [dogs](https://www.boulderhumane.org/animals/adoption/dogs), [cats](https://www.boulderhumane.org/animals/adoption/cats), and [small animals](https://www.boulderhumane.org/animals/adoption/adopt_other) currently up for adoption at the Boulder Humane Society."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the feature of these pages which we are exploiting is their repeated HTML structure. Every animal listed has the following HTML variant:\n",
    "```{html}\n",
    "<div class=\"views-row ... \">\n",
    "  ...\n",
    "  <div class=\"views-field views-field-field-pp-animalname\">\n",
    "    <div class=\"field-content\">\n",
    "      <a href=\"/animals/adoption/\" title=\"Adopt Me!\">Romeo</a>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-primarybreed\">\n",
    "    <div class=\"field-content\">New Zealand</div>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-secondarybreed\">\n",
    "    <div class=\"field-content\">Rabbit</div>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-age\">\n",
    "    ...\n",
    "    <span class=\"field-content\">0 years 2 months</span>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-gender\">\n",
    "    ...\n",
    "    <span class=\"field-content\">Male</span>\n",
    "  </div>\n",
    "  ...\n",
    "</div>\n",
    "``` \n",
    "So to get at the HTML object for each pet, we can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pets = soup.find_all('div', {'class': re.compile('.*views-row.*')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, find all of the `div` tags with the `class` attribute which contains the string `views-row`. \n",
    "\n",
    "Next to grab the name, breeds, and ages of these pets, we’ll grab the children of each pet HTML object. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audi Belgian Malinois  Age:5 years 7 months\n",
      "Kobe Cane Corso  Age:1 year 8 months\n",
      "Roxy Terrier, American Pit Bull Mix Age:1 year 6 months\n",
      "Bear Retriever, Chesapeake Bay Mix Age:2 years 5 months\n",
      "Chloe Akita Mix Age:5 years 0 months\n",
      "Drew Retriever, Labrador Retriever, Golden Age:2 years 0 months\n",
      "Harley Terrier, American Pit Bull Mix Age:2 years 0 months\n",
      "Sadie Great Dane Retriever, Labrador Age:8 years 11 months\n",
      "Megrita Dutch Shepherd Mix Age:5 years 0 months\n",
      "Molly Mastiff Rottweiler Age:8 years 0 months\n",
      "Butterbean Terrier, Jack Russell Mix Age:7 years 0 months\n",
      "Rowdy Rottweiler Mix Age:6 years 0 months\n",
      "Marco Spaniel, American Cocker  Age:5 years 0 months\n",
      "Eva Boxer Mix Age:6 years 0 months\n",
      "Stew Chihuahua, Short Coat Mix Age:7 years 0 months\n",
      "Tom Beagle Mix Age:10 years 0 months\n",
      "Jenny Coonhound, Black and Tan  Age:6 years 3 months\n",
      "Sandy May Coonhound Mix Age:7 years 0 months\n",
      "Bruno Vizsla, Smooth Haired Beagle Age:1 year 6 months\n",
      "Fen Pointer Mix Age:1 year 7 months\n",
      "Elijah Terrier, American Pit Bull Mix Age:0 years 8 months\n",
      "Madre Terrier, American Pit Bull Mix Age:3 years 1 month\n",
      "Ben Shih Tzu Mix Age:2 years 3 months\n",
      "Lulu Boxer Mix Age:4 years 6 months\n",
      "Sahara Siberian Husky Mix Age:2 years 0 months\n",
      "Porter Australian Cattle Dog  Age:3 years 0 months\n",
      "Penny Retriever, Labrador Mix Age:1 year 0 months\n",
      "Baloo Beagle Mix Age:0 years 8 months\n",
      "Kira Australian Cattle Dog Mix Age:1 year 1 month\n",
      "Rocky Retriever, Labrador Mix Age:3 years 1 month\n",
      "Rosco German Shepherd Siberian Husky Age:2 years 6 months\n",
      "Napoleon Boxer Mix Age:0 years 6 months\n",
      "Poppy Siberian Husky  Age:1 year 6 months\n",
      "Willow Australian Shepherd Mix Age:3 years 0 months\n",
      "Patty Chihuahua, Short Coat Dachshund, Miniature Smooth Haired Age:0 years 6 months\n",
      "Suzie Australian Shepherd Mix Age:0 years 2 months\n",
      "Gracie Border Collie Mix Age:0 years 4 months\n",
      "Scooter Australian Cattle Dog Mix Age:0 years 3 months\n",
      "Koda Siberian Husky  Age:2 years 6 months\n",
      "Willow Retriever, Labrador Mix Age:0 years 6 months\n"
     ]
    }
   ],
   "source": [
    "head = \"views-field views-field-field-pp-\"\n",
    "for pet in pets:\n",
    "    name = pet.find('div', {'class': head + 'animalname'}).get_text(strip=True)\n",
    "    primary_breed = pet.find('div', {'class': head + 'primarybreed'}).get_text(strip=True)\n",
    "    secondary_breed = pet.find('div', {'class': head + 'secondarybreed'}).get_text(strip=True)\n",
    "    age = pet.find('div', {'class': head + 'age'}).get_text(strip=True)\n",
    "    print(name, primary_breed, secondary_breed, age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where each call to `find` is getting the children of a pet object, in particular, the `div`s with `class` attributes which look like `views-field views-field-field-pp-*`. Feel free to replace the above code with the cat or small animal pages provided and see how the output changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic (Javascript) Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we requested webpages that required no [Javascript](https://en.wikipedia.org/wiki/JavaScript). In other words, there was no input required on the users' end to view the content of the page (e.g. a login). Let's try a more complicated example of webscraping where content is loaded dynamically.\n",
    "\n",
    "Some characteristics of HTML scraping with [Selenium](https://www.seleniumhq.org/download/) it: (b) can handle javascript, (c) get **HTML** back after the Javascript has been rendered, (d) can behave like a person, though it (a) can be slow. \n",
    "\n",
    "Requirements (one of the below):\n",
    "- Firefox + geckodriver (https://github.com/mozilla/geckodriver/releases)\n",
    "- Chrome + chromedriver (https://sites.google.com/a/chromium.org/chromedriver/)\n",
    "    \n",
    "Note: geckodriver/chromedriver must have execution permissions (chmod +x geckodriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import selenium.webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the browser and define how much are you willing to wait for a page to load. (Many times this is not needed but it doesn't hurt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open the driver (change the executable path to geckodriver_mac.exe or geckodriver.exe)\n",
    "driver = selenium.webdriver.Chrome(executable_path=\"./chromedriver\")\n",
    "#driver = selenium.webdriver.Chrome()\n",
    "driver.implicitly_wait(10)\n",
    "driver.set_page_load_timeout(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit [xkcd](https://xkcd.com) and click through the comics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the xkcd website\n",
    "driver.get(\"https://xkcd.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's find the 'random' buttom\n",
    "element = driver.find_element_by_xpath('//*[@id=\"middleContainer\"]/ul[1]/li[3]/a')\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find an attribute of this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Is he ALSO wondering at what point our thoughts diverged, if they even have yet? 'dude, I think he just took your credit card' AM I THE ORIGINAL? HOW DO I TELL?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element = driver.find_element_by_xpath('//*[@id=\"comic\"]/img')\n",
    "element.get_attribute(\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visit a webpage which requires a login. Signing in to Facebook ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##DO NOT WRITE YOUR PASSWORD IN NOTEBOOKS!!\n",
    "fb_email, fb_pass = \"f1692418@mvrht.com\",\"pedropalotes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Go to Facebook\n",
    "driver.get(\"https://www.facebook.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Send email and password\n",
    "driver.find_element_by_xpath('//*[@id=\"email\"]').send_keys(fb_email)\n",
    "driver.find_element_by_xpath('//*[@id=\"pass\"]').send_keys(fb_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Click on login\n",
    "driver.find_element_by_xpath('//*[@id=\"loginbutton\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find JP\n",
    "element = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div/div[1]/div/div/div/div[1]/div[2]/div/form/div/div/div/div/input[2]')\n",
    "element.send_keys(\"john paul gonzales\")\n",
    "element = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div/div[1]/div/div/div/div[1]/div[2]/div/form/button')\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Click on him\n",
    "element = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[1]/div/div[3]/div[2]/div/div/div[3]/div/div/div/div[1]/div/div/div/div/div/div[2]/div[1]/div/div[1]/a/div')\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Send a friend request (only run during workshop)\n",
    "element = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[1]/div/div[2]/div[2]/div[2]/div/div[1]/div/div[4]/div/div[2]/div/div[2]/span/span/span[1]/a')\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Always remember to close your browser!\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow users to access large amounts of data, companies may provide an [Application Programming Interface (API)](https://en.wikipedia.org/wiki/Application_programming_interface). Often these request are handled via PUT and POST HTTP requests. For example, to make a request from the Twitter API:\n",
    "\n",
    "```{bash}\n",
    "curl --request GET \n",
    " --url 'https://api.twitter.com/1.1/search/tweets.json?q=nasa&result_type=popular' \n",
    " --header 'authorization: OAuth oauth_consumer_key=\"consumer-key-for-app\", ... , \n",
    " oauth_token=\"access-token-for-authed-user\", oauth_version=\"1.0\"'\n",
    " ```\n",
    "\n",
    "APIs often return data in the format of [Javascript Object Notation (JSON)](https://en.wikipedia.org/wiki/JSON). For example:\n",
    "\n",
    "```{json}\n",
    "{\"status\": 200, \"message\": \"hello world\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Hidden\" APIs\n",
    "\n",
    "First, let's try and access what we are calling a \"hidden\" API. That is, we investigate the resources requested by a webpage (e.g. a list of faculty), and make requests directly to that API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_names(letters):\n",
    "    params = (\n",
    "        ('name', letters),\n",
    "        ('request_num', '1'),\n",
    "    )\n",
    "\n",
    "    response = requests.get('https://www.uvm.edu/directory/api/query_results.php', params=params)\n",
    "    if response.ok == True:\n",
    "        return response.text\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = get_names(\"ab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response_json = json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affiliate asender@uvm.edu Abraham Isaac Sender\n",
      "Faculty acrocker@uvm.edu Abigail Miles Crocker\n",
      "Faculty aambaye@uvm.edu Abiy B. Ambaye\n",
      "Staff aldattil@uvm.edu Abbey L. Peterson\n",
      "Faculty avan@uvm.edu Abby Katrien van den Berg\n",
      "Faculty amcgowan@uvm.edu Abigail S. McGowan\n",
      "Student afsmith@uvm.edu Abigayle Frances Smith\n",
      "Student alangsne@uvm.edu Abigail Elizabeth Langsner\n",
      "Faculty atangada@uvm.edu Abhilasha Tangada\n",
      "Faculty awoodhea@uvm.edu Abigail Rhodes Adler\n"
     ]
    }
   ],
   "source": [
    "for i, person in enumerate(response_json[\"data\"]):\n",
    "    if i == 10: \n",
    "        break # Make sure we don't print too much\n",
    "        \n",
    "    print(person[\"edupersonprimaryaffiliation\"][\"0\"], person[\"edupersonprincipalname\"][\"0\"], person[\"cn\"][\"0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit APIs\n",
    "\n",
    "Next, let's try a more typical example of an API. The perks of this approach: (a) send a request and get back JSON, (b) chances are that somebody else has created a Python wrapper for you, but keep in mind that (c) APIs have limits.\n",
    "\n",
    "Let's consider a common API example -- Twitter. To get started:\n",
    "- Get a key: https://apps.twitter.com/\n",
    "- Documentation: https://dev.twitter.com/rest/public\n",
    "- Find a library: https://dev.twitter.com/resources/twitter-libraries (We'll use https://github.com/tweepy/tweepy)\n",
    "\n",
    "Limitations: 100 messages / query, 180 messages every 15 min, & only the last seven days of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /anaconda2/lib/python2.7/site-packages (3.6.0)\n",
      "Requirement already satisfied: requests>=2.11.1 in /anaconda2/lib/python2.7/site-packages (from tweepy) (2.18.4)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /anaconda2/lib/python2.7/site-packages (from tweepy) (1.6.7)\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda2/lib/python2.7/site-packages (from tweepy) (1.11.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /anaconda2/lib/python2.7/site-packages (from tweepy) (1.0.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda2/lib/python2.7/site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda2/lib/python2.7/site-packages (from requests>=2.11.1->tweepy) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda2/lib/python2.7/site-packages (from requests>=2.11.1->tweepy) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda2/lib/python2.7/site-packages (from requests>=2.11.1->tweepy) (2018.4.16)\n",
      "Requirement already satisfied: oauthlib>=0.6.2 in /anaconda2/lib/python2.7/site-packages (from requests-oauthlib>=0.7.0->tweepy) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________\n",
      "sfiscience None 26156\n",
      "Seats are sold out, but you can catch the livestream on our YouTube channel: https://t.co/Sf8LLAYOxk https://t.co/6O2gFDc03D\n",
      "__________\n",
      "sfiscience None 26156\n",
      "We're hiring a full-time social media specialist. This is one of the most dynamic positions @sfiscience. https://t.co/MBgWWTjUNC\n",
      "__________\n",
      "sfiscience None 26156\n",
      "Happening now at SFI: workshop brings together thinkers in disciplines ranging from cosmology to chronobiology to n… https://t.co/NCt5Fdtf2e\n",
      "__________\n",
      "sfiscience None 26156\n",
      "@NigelGoldenfeld Please present this next time you come to @sfiscience!\n",
      "__________\n",
      "sfiscience None 26156\n",
      "RT @NigelGoldenfeld: Finally out after 8 years of work!  Experimental observation of stochastic Turing patterns, in a forward-engineered bi…\n",
      "__________\n",
      "sfiscience None 26156\n",
      "RT @brianjdermody: Fantastic workshop on the complexities of human-water systems @sfiscience. Sooo many new ideas, gonna take some time to…\n",
      "__________\n",
      "sfiscience None 26156\n",
      "Congratulations indeed! https://t.co/yccaxYOMJk\n",
      "__________\n",
      "sfiscience None 26156\n",
      "Catch last Tuesday's interview about emerging media arts on KSFR  https://t.co/Klo1YXnH9q\n",
      "__________\n",
      "sfiscience None 26156\n",
      "Santa Fe's EMA launch party is tonight! The Emerging Media Alliance \"celebrates a new era of teamwork and achieveme… https://t.co/U1AsztM5fe\n",
      "__________\n",
      "sfiscience None 26156\n",
      "Genes of modern males suggest a mass die-off of men 5-7 thousand years ago. An analysis by External Professor Marc… https://t.co/0dWENIOGEy\n",
      "__________\n",
      "sfiscience None 26156\n",
      "The story features seminal research by SFI's Geoffrey West, Brian Enquist, and James Brown, published in Science in… https://t.co/jsRWoiu7Op\n",
      "__________\n",
      "sfiscience None 26156\n",
      "Neat story by @vcallier in @QuantaMagazine on the biophysics underlying the powerful jumps, punches, and bites in s… https://t.co/NwG9MRMSIu\n",
      "__________\n",
      "sfiscience None 26156\n",
      "Axle Contemporary, a mobile exhibit space based in Santa Fe, premiered a new experiential piece during… https://t.co/uKX0A8a6MA\n",
      "__________\n",
      "sfiscience None 26156\n",
      "Happening now at SFI: former SFI postdocs @MarionDumas1 and Christa Brelsford lead the Socio-Hydrological Dynamics… https://t.co/4GcOcXx2aG\n",
      "__________\n",
      "sfiscience None 26156\n",
      "@mcmsc01 @INSPI_ECUADOR @ASU @ASUBeingHuman @inguniandes @UNColombia @rhidalgott @VarsoviaC @DanLarremore\n",
      "__________\n",
      "sfiscience None 26156\n",
      "The kanji for \"beauty\" and address of the Bitcoin genesis block, drawn by @TuurDemeester, appear below the Gell-Mann equation.\n",
      "__________\n",
      "sfiscience None 26156\n",
      "The M Alpha Monolith now stands on SFI's Hyde Park campus. Etched in the stone, the glyph for \"M\" + Greek alpha ack… https://t.co/1uMUPfVRzG\n",
      "__________\n",
      "sfiscience None 26156\n",
      "The InterPlanetary Festival was the first event in a series that offers \"a new way of seeing.\"  @ABQJournal reports… https://t.co/csJToIgcWh\n",
      "__________\n",
      "sfiscience None 26156\n",
      "@julioccneto @DavidFeldman They are not, but you can take Liz Bradley's course in Nonlinear Dynamics through… https://t.co/KcUbXOvtrV\n",
      "__________\n",
      "sfiscience None 26156\n",
      "Come back anytime! https://t.co/PRJg7rDfEV\n",
      "__________\n",
      "sfiscience None 26156\n",
      "RT @TuurDemeester: After attending the inimitable  @IP_Fest, I was honored to be with @B3_MillerValue for the inauguration of the M Alpha M…\n",
      "__________\n",
      "sfiscience None 26156\n",
      "@mcmsc01 @MantillaIgnacio @michaelcrow @stevenstrogatz @cparedesverduga @rhidalgott @cmontufar @esantos1957… https://t.co/uUt7Cgwt9v\n",
      "Number of results: 22 (22 new)\n",
      "Number of results: 22 (0 new)\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "def twitter(d_keys,query):\n",
    "    #Authtentify\n",
    "    auth = tweepy.OAuthHandler(d_keys[\"CONSUMER_KEY\"], d_keys[\"CONSUMER_SECRET\"])\n",
    "    auth.set_access_token(d_keys[\"ACCESS_KEY\"], d_keys[\"ACCESS_SECRET\"])\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    # We want 1000 tweets\n",
    "    num_results = 1000\n",
    "    result_count = 0\n",
    "    last_id = None\n",
    "    \n",
    "    # Max 180 tweets 15 min\n",
    "    cumulative = 0\n",
    "\n",
    "    #While we don't have them\n",
    "    while (result_count <  num_results):\n",
    "        previous_tweets = result_count\n",
    "        # Ask for more tweets, starting in the 'last_id' (identifier of the tweet)\n",
    "        results = api.search(q = query,\n",
    "                              count = 90, max_id = last_id, result_type=\"recent\")\n",
    "                                # geocode = \"{},{},{}km\".format(latitude, longitude, max_range) #for geocode\n",
    "\n",
    "        # For each tweet extract some info (JSON structure)\n",
    "        for result in results:\n",
    "            result_count += 1\n",
    "            user = result.user.screen_name\n",
    "            text = result.text\n",
    "            followers_count = result.user.followers_count\n",
    "            time_zone = result.user.time_zone\n",
    "            print(\"_\"*10)\n",
    "            print(user,time_zone,followers_count)\n",
    "            print(text)\n",
    "\n",
    "        # Keep the last_id to know where to continue\n",
    "        last_id = int(result.id)-1\n",
    "        new_tweets = result_count - previous_tweets\n",
    "\n",
    "        print (\"Number of results: {} ({} new)\".format(result_count,new_tweets))\n",
    "\n",
    "        # If we don't get new tweets exit\n",
    "        if new_tweets == 0: \n",
    "            break\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        if ((result_count + 90) // 150) > cumulative:\n",
    "            cumulative += 1\n",
    "            time.sleep(15*60)\n",
    "\n",
    "\n",
    "d_keys = pickle.load(open(\".key\",\"rb\")) # Don't share your keys ;)\n",
    "twitter(d_keys,\"from:sfiscience\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
