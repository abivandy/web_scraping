{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Digital Trace Data: Web Scraping / APIs\n",
    "June 19th, 2018 - Javier Garcia-Bernardo & Allie Morgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Requirements\n",
    "import requests               # Simple HTTP operations (GET and POST)\n",
    "import selenium               # Loads dynamic (javascript) pages\n",
    "import json                   # Parsing the responses from APIs\n",
    "import re                     # Python library for parsing regular expressions\n",
    "from bs4 import BeautifulSoup # Parsing HTML\n",
    "import pandas as pd           # Read tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Web scraping](https://en.wikipedia.org/wiki/Web_scraping) is a method for extracting data from the web. There are many techniques which can be used for web scraping — ranging from requiring human involvement (“human copy-paste”) to fully automated systems (using computer vision). Somewhere in the middle is HTML parsing, which we will describe here.\n",
    "\n",
    "Web scraping using [HTML parsing](https://en.wikipedia.org/wiki/Web_scraping#HTML_parsing) is often used on webpages which share similar HTML structure. For example, you might want to scrape the ingredients from chocolate chip cookie recipes to identify correlations between ingredients and five-star worthy cookies, or you might want to predict who will win March Madness by looking at game play-by-plays, or you want to know all the local pets up for adoption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<head>\n",
      "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\" />\n",
      "<meta charset=\"utf-8\" />\n",
      "<link rel=\"shortcut icon\" href=\"https://www.boulderhumane.org/sites/default/files/favicon.ico\" type=\"image/vnd.microsoft.icon\" />\n",
      "<meta name=\"Generator\" content=\"Drupal 7 (http://drupal.org)\" />\n",
      "<meta name=\"viewport\" content=\"width=1000px, initial-scale=1.0, maximum-scale=1.0\" />\n",
      "<title>Dogs Available for Adoption | Humane Society of Boulder Valley</title>\n",
      "<link type=\"text/css\" rel=\"stylesheet\n"
     ]
    }
   ],
   "source": [
    "pet_pages = [\"https://www.boulderhumane.org/animals/adoption/dogs\", \n",
    "             \"https://www.boulderhumane.org/animals/adoption/cats\", \n",
    "             \"https://www.boulderhumane.org/animals/adoption/adopt_other\"]\n",
    "\n",
    "r = requests.get(pet_pages[0])\n",
    "html = r.text\n",
    "print(html[:500]) # Print the first 500 characters of the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you visit a webpage, your web browser renders an HTML document with CSS and Javascript to produce a visually appealing page. (See the HTML above.) [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) is a Python library for parsing HTML. We'll use it to extract all of the names, ages, and breeds of the [dogs](https://www.boulderhumane.org/animals/adoption/dogs), [cats](https://www.boulderhumane.org/animals/adoption/cats), and [small animals](https://www.boulderhumane.org/animals/adoption/adopt_other) currently up for adoption at the Boulder Humane Society."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the feature of these pages which we are exploiting is their repeated HTML structure. Every animal listed has the following HTML variant:\n",
    "```{html}\n",
    "<div class=\"views-row ... \">\n",
    "  ...\n",
    "  <div class=\"views-field views-field-field-pp-animalname\">\n",
    "    <div class=\"field-content\">\n",
    "      <a href=\"/animals/adoption/\" title=\"Adopt Me!\">Romeo</a>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-primarybreed\">\n",
    "    <div class=\"field-content\">New Zealand</div>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-secondarybreed\">\n",
    "    <div class=\"field-content\">Rabbit</div>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-age\">\n",
    "    ...\n",
    "    <span class=\"field-content\">0 years 2 months</span>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-gender\">\n",
    "    ...\n",
    "    <span class=\"field-content\">Male</span>\n",
    "  </div>\n",
    "  ...\n",
    "</div>\n",
    "``` \n",
    "So to get at the HTML object for each pet, we can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pets = soup.find_all('div', {'class': re.compile('.*views-row.*')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, find all of the `div` tags with the `class` attribute which contains the string `views-row`. \n",
    "\n",
    "Next to grab the name, breeds, and ages of these pets, we’ll grab the children of each pet HTML object. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kobe Cane Corso  Age:1 year 8 months\n",
      "Roxy Terrier, American Pit Bull Mix Age:1 year 6 months\n",
      "Bear Retriever, Chesapeake Bay Mix Age:2 years 5 months\n",
      "Chloe Akita Mix Age:5 years 0 months\n",
      "Drew Retriever, Labrador Retriever, Golden Age:2 years 0 months\n",
      "Harley Terrier, American Pit Bull Mix Age:2 years 0 months\n",
      "Sadie Great Dane Retriever, Labrador Age:8 years 11 months\n",
      "Megrita Dutch Shepherd Mix Age:5 years 0 months\n",
      "Molly Mastiff Rottweiler Age:8 years 0 months\n",
      "Butterbean Terrier, Jack Russell  Age:7 years 0 months\n",
      "Rowdy Rottweiler Mix Age:6 years 0 months\n",
      "Marco Spaniel, American Cocker  Age:5 years 0 months\n",
      "Eva Boxer Mix Age:6 years 0 months\n",
      "Stew Chihuahua, Short Coat Mix Age:7 years 0 months\n",
      "Rudy Doberman Pinscher  Age:3 years 2 months\n",
      "Tom Beagle  Age:10 years 0 months\n",
      "Walnut Terrier, American Pit Bull Mix Age:4 years 0 months\n",
      "Sandy May Coonhound Mix Age:7 years 0 months\n",
      "Bruno Vizsla, Smooth Haired Beagle Age:1 year 6 months\n",
      "Fen Pointer Mix Age:1 year 7 months\n",
      "Madre Terrier, American Pit Bull Mix Age:3 years 1 month\n",
      "Ben Shih Tzu Mix Age:2 years 3 months\n",
      "Lulu Boxer Mix Age:4 years 6 months\n",
      "Sahara Siberian Husky Mix Age:2 years 0 months\n",
      "Porter Australian Cattle Dog  Age:3 years 0 months\n",
      "Baloo Beagle Mix Age:0 years 8 months\n",
      "Kira Australian Cattle Dog Mix Age:1 year 1 month\n",
      "Rocky Retriever, Labrador Mix Age:3 years 1 month\n",
      "Charley Terrier, Jack Russell Mix Age:0 years 2 months\n",
      "Napoleon Boxer Mix Age:0 years 6 months\n",
      "Abigail Retriever, Labrador Mix Age:0 years 3 months\n",
      "June Retriever, Labrador Mix Age:0 years 3 months\n",
      "Poppy Siberian Husky  Age:1 year 6 months\n",
      "Zeke Mastiff Mix Age:0 years 2 months\n",
      "Charlie German Shepherd Mix Age:0 years 3 months\n",
      "Chance Schnauzer, Miniature Mix Age:0 years 3 months\n",
      "Chester German Shepherd Mix Age:0 years 3 months\n",
      "Willow Australian Shepherd Mix Age:3 years 0 months\n",
      "Hershey German Shepherd Mix Age:0 years 8 months\n",
      "Chico Terrier, Jack Russell Mix Age:0 years 2 months\n",
      "Jazzy Schnauzer, Miniature Mix Age:1 year 0 months\n",
      "Patty Chihuahua, Short Coat Dachshund, Miniature Smooth Haired Age:0 years 6 months\n",
      "Suzie Australian Shepherd Mix Age:0 years 2 months\n",
      "Randy Terrier, American Pit Bull Mix Age:0 years 2 months\n",
      "Blossom Retriever, Labrador Mix Age:0 years 2 months\n",
      "Bonnie Retriever, Labrador Mix Age:0 years 2 months\n",
      "Donnie Retriever, Labrador Mix Age:0 years 2 months\n",
      "Clyde Retriever, Labrador Mix Age:0 years 2 months\n",
      "Gracie Border Collie Mix Age:0 years 4 months\n",
      "Koda Siberian Husky  Age:2 years 6 months\n",
      "Willow Retriever, Labrador Mix Age:0 years 6 months\n",
      "Bernie Canaan Mix Age:3 years 0 months\n",
      "Taz Australian Cattle Dog Mix Age:0 years 7 months\n",
      "Charley Spaniel, American Cocker Papillon Age:5 years 6 months\n",
      "Princess Poodle, Miniature  Age:4 years 1 month\n",
      "Angel Terrier, American Pit Bull Mix Age:3 years 2 months\n"
     ]
    }
   ],
   "source": [
    "head = \"views-field views-field-field-pp-\"\n",
    "for pet in pets:\n",
    "    name = pet.find('div', {'class': head + 'animalname'}).get_text(strip=True)\n",
    "    primary_breed = pet.find('div', {'class': head + 'primarybreed'}).get_text(strip=True)\n",
    "    secondary_breed = pet.find('div', {'class': head + 'secondarybreed'}).get_text(strip=True)\n",
    "    age = pet.find('div', {'class': head + 'age'}).get_text(strip=True)\n",
    "    print(name, primary_breed, secondary_breed, age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where each call to `find` is getting the children of a pet object, in particular, the `div`s with `class` attributes which look like `views-field views-field-field-pp-*`. Feel free to replace the above code with the cat or small animal pages provided and see how the output changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: Read Tables from Webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Image</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bacon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Often eaten with ketchup or brown sauce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bacon, egg and cheese</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Breakfast sandwich, usually with fried or scra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bagel toast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Israel</td>\n",
       "      <td>Pressed, toasted bagel filled with vegetables ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Baked bean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Canned baked beans on white or brown bread, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bánh mì[4]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Filling is typically meat, but can contain a w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Barbecue[5][6][7]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Served on a bun, with chopped, sliced, or shre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Barros Jarpa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Ham and cheese, usually mantecoso, which is si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Barros Luco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Beef (usually thin-cut steak) and cheese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bauru</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Melted cheese, roast beef, tomato, and pickled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Beef on weck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States (Buffalo, New York)</td>\n",
       "      <td>Roast beef on a Kummelweck roll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Beirute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Melted cheese, sliced fresh tomatoes with oreg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BLT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Named for its ingredients: bacon, lettuce, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bocadillo de calamares</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Baguette bread filled with fried squid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bologna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States, Canada</td>\n",
       "      <td>Pre-sliced and sometimes fried bologna sausage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Boloney (Bologna) salad sandwich</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE Pennsylvania</td>\n",
       "      <td>A mixture of bologna sausage and sweet gherkin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bosna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Usually grilled on white bread, containing a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bratwurst Sandwich</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany</td>\n",
       "      <td>The Bratwurst sandwich is a popular street foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Breakfast roll</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom and Ireland</td>\n",
       "      <td>Convenience dish on a variety of bread rolls, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Breakfast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Typically a scrambled or fried egg, cheese, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>British Rail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Reference to the poor quality of catering on t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Name  Image  \\\n",
       "0                              Bacon    NaN   \n",
       "1              Bacon, egg and cheese    NaN   \n",
       "2                        Bagel toast    NaN   \n",
       "3                         Baked bean    NaN   \n",
       "4                         Bánh mì[4]    NaN   \n",
       "5                  Barbecue[5][6][7]    NaN   \n",
       "6                       Barros Jarpa    NaN   \n",
       "7                        Barros Luco    NaN   \n",
       "8                              Bauru    NaN   \n",
       "9                       Beef on weck    NaN   \n",
       "10                           Beirute    NaN   \n",
       "11                               BLT    NaN   \n",
       "12            Bocadillo de calamares    NaN   \n",
       "13                           Bologna    NaN   \n",
       "14  Boloney (Bologna) salad sandwich    NaN   \n",
       "15                             Bosna    NaN   \n",
       "16                Bratwurst Sandwich    NaN   \n",
       "17                    Breakfast roll    NaN   \n",
       "18                         Breakfast    NaN   \n",
       "19                      British Rail    NaN   \n",
       "\n",
       "                               Origin  \\\n",
       "0                      United Kingdom   \n",
       "1                       United States   \n",
       "2                              Israel   \n",
       "3                       United States   \n",
       "4                             Vietnam   \n",
       "5                       United States   \n",
       "6                               Chile   \n",
       "7                               Chile   \n",
       "8                              Brazil   \n",
       "9   United States (Buffalo, New York)   \n",
       "10                             Brazil   \n",
       "11                      United States   \n",
       "12                              Spain   \n",
       "13              United States, Canada   \n",
       "14                    NE Pennsylvania   \n",
       "15                            Austria   \n",
       "16                            Germany   \n",
       "17         United Kingdom and Ireland   \n",
       "18                      United States   \n",
       "19                     United Kingdom   \n",
       "\n",
       "                                          Description  \n",
       "0             Often eaten with ketchup or brown sauce  \n",
       "1   Breakfast sandwich, usually with fried or scra...  \n",
       "2   Pressed, toasted bagel filled with vegetables ...  \n",
       "3   Canned baked beans on white or brown bread, so...  \n",
       "4   Filling is typically meat, but can contain a w...  \n",
       "5   Served on a bun, with chopped, sliced, or shre...  \n",
       "6   Ham and cheese, usually mantecoso, which is si...  \n",
       "7            Beef (usually thin-cut steak) and cheese  \n",
       "8   Melted cheese, roast beef, tomato, and pickled...  \n",
       "9                     Roast beef on a Kummelweck roll  \n",
       "10  Melted cheese, sliced fresh tomatoes with oreg...  \n",
       "11  Named for its ingredients: bacon, lettuce, and...  \n",
       "12             Baguette bread filled with fried squid  \n",
       "13  Pre-sliced and sometimes fried bologna sausage...  \n",
       "14  A mixture of bologna sausage and sweet gherkin...  \n",
       "15  Usually grilled on white bread, containing a b...  \n",
       "16  The Bratwurst sandwich is a popular street foo...  \n",
       "17  Convenience dish on a variety of bread rolls, ...  \n",
       "18  Typically a scrambled or fried egg, cheese, an...  \n",
       "19  Reference to the poor quality of catering on t...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_sandwiches\",header=0)[0]\n",
    "\n",
    "# Write table to CSV\n",
    "#table.to_csv(\"filenamehere.csv\")\n",
    "\n",
    "table.head((20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic (Javascript) Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we requested webpages that required no [Javascript](https://en.wikipedia.org/wiki/JavaScript). In other words, there was no input required on the users' end to view the content of the page (e.g. a login). Let's try a more complicated example of webscraping where content is loaded dynamically.\n",
    "\n",
    "Some characteristics of HTML scraping with [Selenium](https://www.seleniumhq.org/download/) it: (b) can handle javascript, (c) get **HTML** back after the Javascript has been rendered, (d) can behave like a person, though it (a) can be slow. \n",
    "\n",
    "Requirements (one of the below):\n",
    "- Firefox + geckodriver (https://github.com/mozilla/geckodriver/releases)\n",
    "- Chrome + chromedriver (https://sites.google.com/a/chromium.org/chromedriver/)\n",
    "    \n",
    "Note: geckodriver/chromedriver must have execution permissions (chmod +x geckodriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import selenium.webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the browser and define how much are you willing to wait for a page to load. (Many times this is not needed but it doesn't hurt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the driver (change the executable path to geckodriver_mac.exe or geckodriver.exe)\n",
    "driver = selenium.webdriver.Chrome(executable_path=\"./chromedriver\")\n",
    "#driver = selenium.webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit [xkcd](https://xkcd.com) and click through the comics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the xkcd website\n",
    "driver.get(\"https://xkcd.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's find the 'random' buttom\n",
    "element = driver.find_element_by_xpath('//*[@id=\"middleContainer\"]/ul[1]/li[3]/a')\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find an attribute of this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Although the Markov chain-style text model is still rudimentary; it recently gave me \"Massachusetts Institute of America\". Although I have to admit it sounds prestigious.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element = driver.find_element_by_xpath('//*[@id=\"comic\"]/img')\n",
    "element.get_attribute(\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"middleContainer\"]/ul[1]/li[3]/a').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visit a webpage which requires a login. Signing in to Facebook ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##DO NOT WRITE YOUR PASSWORD IN NOTEBOOKS!!\n",
    "fb_email, fb_pass = \"<email>\", \"<password>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Go to Facebook\n",
    "driver.get(\"https://www.facebook.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send email and password\n",
    "driver.find_element_by_xpath('//*[@id=\"email\"]').send_keys(fb_email)\n",
    "driver.find_element_by_xpath('//*[@id=\"pass\"]').send_keys(fb_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Click on login\n",
    "driver.find_element_by_xpath('//*[@id=\"loginbutton\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find JP\n",
    "element = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div/div[1]/div/div/div/div[1]/div[2]/div/form/div/div/div/div/input[2]')\n",
    "element.send_keys(\"<friend's name>\")\n",
    "element = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div/div[1]/div/div/div/div[1]/div[2]/div/form/button')\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Click on him\n",
    "element = driver.find_element_by_xpath('//*[@class=\"_2yet\"]')\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Send a friend request (only run during workshop)\n",
    "element = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[1]/div/div[2]/div[2]/div[2]/div/div[1]/div/div[4]/div/div[2]/div/div[2]/span/span/span[1]/a')\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Always remember to close your browser!\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow users to access large amounts of data, companies may provide an [Application Programming Interface (API)](https://en.wikipedia.org/wiki/Application_programming_interface). Often these request are handled via PUT and POST HTTP requests. For example, to make a request from the Twitter API:\n",
    "\n",
    "```{bash}\n",
    "curl --request GET \n",
    " --url 'https://api.twitter.com/1.1/search/tweets.json?q=nasa&result_type=popular' \n",
    " --header 'authorization: OAuth oauth_consumer_key=\"consumer-key-for-app\", ... , \n",
    " oauth_token=\"access-token-for-authed-user\", oauth_version=\"1.0\"'\n",
    " ```\n",
    "\n",
    "APIs often return data in the format of [Javascript Object Notation (JSON)](https://en.wikipedia.org/wiki/JSON). For example:\n",
    "\n",
    "```{json}\n",
    "{\"status\": 200, \"message\": \"hello world\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Hidden\" APIs\n",
    "\n",
    "First, let's try and access what we are calling a \"hidden\" API. That is, we investigate the resources requested by a webpage (e.g. a list of faculty), and make requests directly to that API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_names(letters):\n",
    "    params = (\n",
    "        ('name', letters),\n",
    "        ('request_num', '1'),\n",
    "    )\n",
    "\n",
    "    response = requests.get('https://www.uvm.edu/directory/api/query_results.php', params=params)\n",
    "    if response.ok == True:\n",
    "        return response.text\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = get_names(\"ab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response_json = json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, person in enumerate(response_json[\"data\"]):\n",
    "    if i == 10: \n",
    "        break # Make sure we don't print too much\n",
    "        \n",
    "    #print(person[\"edupersonprimaryaffiliation\"][\"0\"], person[\"edupersonprincipalname\"][\"0\"], person[\"cn\"][\"0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit APIs\n",
    "\n",
    "Next, let's try a more typical example of an API. The perks of this approach: (a) send a request and get back JSON, (b) chances are that somebody else has created a Python wrapper for you, but keep in mind that (c) APIs have limits.\n",
    "\n",
    "Let's consider a common API example -- Twitter. To get started:\n",
    "- Get a key: https://apps.twitter.com/\n",
    "- Documentation: https://dev.twitter.com/rest/public\n",
    "- Find a library: https://dev.twitter.com/resources/twitter-libraries (We'll use https://github.com/tweepy/tweepy)\n",
    "\n",
    "Limitations: 100 messages / query, 180 messages every 15 min, & only the last seven days of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /anaconda2/lib/python2.7/site-packages (3.6.0)\n",
      "Requirement already satisfied: requests>=2.11.1 in /anaconda2/lib/python2.7/site-packages (from tweepy) (2.18.4)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /anaconda2/lib/python2.7/site-packages (from tweepy) (1.6.7)\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda2/lib/python2.7/site-packages (from tweepy) (1.11.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /anaconda2/lib/python2.7/site-packages (from tweepy) (1.0.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda2/lib/python2.7/site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda2/lib/python2.7/site-packages (from requests>=2.11.1->tweepy) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda2/lib/python2.7/site-packages (from requests>=2.11.1->tweepy) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda2/lib/python2.7/site-packages (from requests>=2.11.1->tweepy) (2018.4.16)\n",
      "Requirement already satisfied: oauthlib>=0.6.2 in /anaconda2/lib/python2.7/site-packages (from requests-oauthlib>=0.7.0->tweepy) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________\n",
      "sfiscience None 26176\n",
      "Live now: panel discussion on the nature of time with David Krakauer, Jim Hartle, and @seanmcarroll, moderated by s… https://t.co/KjEdjIoYJv\n",
      "__________\n",
      "sfiscience None 26176\n",
      "RT @michaelgarfield: All the panel discussion live notes, 360º videos, and musical recordings I made at InterPlanetary Festival last week:…\n",
      "__________\n",
      "sfiscience None 26176\n",
      "Enroll now for @ComplexExplorer's MOOC: Introduction to Agent-based Modeling with Bill Rand. Students learn to crea… https://t.co/BSz6XqxsPN\n",
      "__________\n",
      "sfiscience None 26176\n",
      "Seats are sold out, but you can catch the livestream on our YouTube channel: https://t.co/Sf8LLAYOxk https://t.co/6O2gFDc03D\n",
      "__________\n",
      "sfiscience None 26176\n",
      "We're hiring a full-time social media specialist. This is one of the most dynamic positions @sfiscience. https://t.co/MBgWWTjUNC\n",
      "__________\n",
      "sfiscience None 26176\n",
      "Happening now at SFI: workshop brings together thinkers in disciplines ranging from cosmology to chronobiology to n… https://t.co/NCt5Fdtf2e\n",
      "__________\n",
      "sfiscience None 26176\n",
      "@NigelGoldenfeld Please present this next time you come to @sfiscience!\n",
      "__________\n",
      "sfiscience None 26176\n",
      "RT @NigelGoldenfeld: Finally out after 8 years of work!  Experimental observation of stochastic Turing patterns, in a forward-engineered bi…\n",
      "__________\n",
      "sfiscience None 26176\n",
      "RT @brianjdermody: Fantastic workshop on the complexities of human-water systems @sfiscience. Sooo many new ideas, gonna take some time to…\n",
      "__________\n",
      "sfiscience None 26176\n",
      "Congratulations indeed! https://t.co/yccaxYOMJk\n",
      "__________\n",
      "sfiscience None 26176\n",
      "Catch last Tuesday's interview about emerging media arts on KSFR  https://t.co/Klo1YXnH9q\n",
      "__________\n",
      "sfiscience None 26176\n",
      "Santa Fe's EMA launch party is tonight! The Emerging Media Alliance \"celebrates a new era of teamwork and achieveme… https://t.co/U1AsztM5fe\n",
      "__________\n",
      "sfiscience None 26176\n",
      "Genes of modern males suggest a mass die-off of men 5-7 thousand years ago. An analysis by External Professor Marc… https://t.co/0dWENIOGEy\n",
      "__________\n",
      "sfiscience None 26176\n",
      "The story features seminal research by SFI's Geoffrey West, Brian Enquist, and James Brown, published in Science in… https://t.co/jsRWoiu7Op\n",
      "__________\n",
      "sfiscience None 26176\n",
      "Neat story by @vcallier in @QuantaMagazine on the biophysics underlying the powerful jumps, punches, and bites in s… https://t.co/NwG9MRMSIu\n",
      "__________\n",
      "sfiscience None 26176\n",
      "Axle Contemporary, a mobile exhibit space based in Santa Fe, premiered a new experiential piece during… https://t.co/uKX0A8a6MA\n",
      "__________\n",
      "sfiscience None 26176\n",
      "Happening now at SFI: former SFI postdocs @MarionDumas1 and Christa Brelsford lead the Socio-Hydrological Dynamics… https://t.co/4GcOcXx2aG\n",
      "__________\n",
      "sfiscience None 26176\n",
      "@mcmsc01 @INSPI_ECUADOR @ASU @ASUBeingHuman @inguniandes @UNColombia @rhidalgott @VarsoviaC @DanLarremore\n",
      "__________\n",
      "sfiscience None 26176\n",
      "The kanji for \"beauty\" and address of the Bitcoin genesis block, drawn by @TuurDemeester, appear below the Gell-Mann equation.\n",
      "__________\n",
      "sfiscience None 26176\n",
      "The M Alpha Monolith now stands on SFI's Hyde Park campus. Etched in the stone, the glyph for \"M\" + Greek alpha ack… https://t.co/1uMUPfVRzG\n",
      "__________\n",
      "sfiscience None 26176\n",
      "The InterPlanetary Festival was the first event in a series that offers \"a new way of seeing.\"  @ABQJournal reports… https://t.co/csJToIgcWh\n",
      "__________\n",
      "sfiscience None 26176\n",
      "@julioccneto @DavidFeldman They are not, but you can take Liz Bradley's course in Nonlinear Dynamics through… https://t.co/KcUbXOvtrV\n",
      "__________\n",
      "sfiscience None 26176\n",
      "Come back anytime! https://t.co/PRJg7rDfEV\n",
      "Number of results: 23 (23 new)\n",
      "Number of results: 23 (0 new)\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "def twitter(d_keys,query):\n",
    "    # Authtentify\n",
    "    auth = tweepy.OAuthHandler(d_keys[\"CONSUMER_KEY\"], d_keys[\"CONSUMER_SECRET\"])\n",
    "    auth.set_access_token(d_keys[\"ACCESS_KEY\"], d_keys[\"ACCESS_SECRET\"])\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    # We want 1000 tweets\n",
    "    num_results = 1000\n",
    "    result_count = 0\n",
    "    last_id = None\n",
    "    \n",
    "    # Max 180 tweets 15 min\n",
    "    cumulative = 0\n",
    "\n",
    "    #While we don't have them\n",
    "    while (result_count <  num_results):\n",
    "        previous_tweets = result_count\n",
    "        # Ask for more tweets, starting in the 'last_id' (identifier of the tweet)\n",
    "        results = api.search(q = query,\n",
    "                              count = 90, max_id = last_id, result_type=\"recent\")\n",
    "                                # geocode = \"{},{},{}km\".format(latitude, longitude, max_range) #for geocode\n",
    "\n",
    "        # For each tweet extract some info (JSON structure)\n",
    "        for result in results:\n",
    "            result_count += 1\n",
    "            user = result.user.screen_name\n",
    "            text = result.text\n",
    "            followers_count = result.user.followers_count\n",
    "            time_zone = result.user.time_zone\n",
    "            print(\"_\"*10)\n",
    "            print(user,time_zone,followers_count)\n",
    "            print(text)\n",
    "\n",
    "        # Keep the last_id to know where to continue\n",
    "        last_id = int(result.id)-1\n",
    "        new_tweets = result_count - previous_tweets\n",
    "\n",
    "        print (\"Number of results: {} ({} new)\".format(result_count,new_tweets))\n",
    "\n",
    "        # If we don't get new tweets exit\n",
    "        if new_tweets == 0: \n",
    "            break\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        if ((result_count + 90) // 150) > cumulative:\n",
    "            cumulative += 1\n",
    "            time.sleep(15*60)\n",
    "\n",
    "\n",
    "d_keys = pickle.load(open(\".key\",\"rb\")) # Don't share your keys ;)\n",
    "twitter(d_keys,\"from:sfiscience\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
