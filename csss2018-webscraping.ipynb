{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Digital Trace Data: Web Scraping / APIs\n",
    "June 19th, 2018 - Javier Garcia-Bernardo & Allie Morgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Requirements\n",
    "import requests               # Simple HTTP operations (GET and POST)\n",
    "import selenium               # Loads dynamic (javascript) pages\n",
    "import json                   # Parsing the responses from APIs\n",
    "import re                     # Python library for parsing regular expressions\n",
    "from bs4 import BeautifulSoup # Parsing HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Web scraping](https://en.wikipedia.org/wiki/Web_scraping) is a method for extracting data from the web. There are many techniques which can be used for web scraping — ranging from requiring human involvement (“human copy-paste”) to fully automated systems (using computer vision). Somewhere in the middle is HTML parsing, which we will describe here.\n",
    "\n",
    "Web scraping using [HTML parsing](https://en.wikipedia.org/wiki/Web_scraping#HTML_parsing) is often used on webpages which share similar HTML structure. For example, you might want to scrape the ingredients from chocolate chip cookie recipes to identify correlations between ingredients and five-star worthy cookies, or you might want to predict who will win March Madness by looking at game play-by-plays, or you want to know all the local pets up for adoption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<head>\n",
      "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\" />\n",
      "<meta charset=\"utf-8\" />\n",
      "<link rel=\"shortcut icon\" href=\"https://www.boulderhumane.org/sites/default/files/favicon.ico\" type=\"image/vnd.microsoft.icon\" />\n",
      "<meta name=\"Generator\" content=\"Drupal 7 (http://drupal.org)\" />\n",
      "<meta name=\"viewport\" content=\"width=1000px, initial-scale=1.0, maximum-scale=1.0\" />\n",
      "<title>Dogs Available for Adoption | Humane Society of Boulder Valley</title>\n",
      "<link type=\"text/css\" rel=\"stylesheet\n"
     ]
    }
   ],
   "source": [
    "pet_pages = [\"https://www.boulderhumane.org/animals/adoption/dogs\", \n",
    "             \"https://www.boulderhumane.org/animals/adoption/cats\", \n",
    "             \"https://www.boulderhumane.org/animals/adoption/adopt_other\"]\n",
    "\n",
    "r = requests.get(pet_pages[0])\n",
    "html = r.text\n",
    "print(html[:500]) # Print the first 500 characters of the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you visit a webpage, your web browser renders an HTML document with CSS and Javascript to produce a visually appealing page. (See the HTML above.) [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) is a Python library for parsing HTML. We'll use it to extract all of the names, ages, and breeds of the [dogs](https://www.boulderhumane.org/animals/adoption/dogs), [cats](https://www.boulderhumane.org/animals/adoption/cats), and [small animals](https://www.boulderhumane.org/animals/adoption/adopt_other) currently up for adoption at the Boulder Humane Society."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the feature of these pages which we are exploiting is their repeated HTML structure. Every animal listed has the following HTML variant:\n",
    "```{html}\n",
    "<div class=\"views-row ... \">\n",
    "  ...\n",
    "  <div class=\"views-field views-field-field-pp-animalname\">\n",
    "    <div class=\"field-content\">\n",
    "      <a href=\"/animals/adoption/\" title=\"Adopt Me!\">Romeo</a>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-primarybreed\">\n",
    "    <div class=\"field-content\">New Zealand</div>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-secondarybreed\">\n",
    "    <div class=\"field-content\">Rabbit</div>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-age\">\n",
    "    ...\n",
    "    <span class=\"field-content\">0 years 2 months</span>\n",
    "  </div>\n",
    "  <div class=\"views-field views-field-field-pp-gender\">\n",
    "    ...\n",
    "    <span class=\"field-content\">Male</span>\n",
    "  </div>\n",
    "  ...\n",
    "</div>\n",
    "``` \n",
    "So to get at the HTML object for each pet, we can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pets = soup.find_all('div', {'class': re.compile('.*views-row.*')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, find all of the `div` tags with the `class` attribute which contains the string `views-row`. \n",
    "\n",
    "Next to grab the name, breeds, and ages of these pets, we’ll grab the children of each pet HTML object. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Audi', u'Belgian Malinois', u'', u'Age:5 years 7 months')\n",
      "(u'Kobe', u'Cane Corso', u'', u'Age:1 year 8 months')\n",
      "(u'Roxy', u'Terrier, American Pit Bull', u'Mix', u'Age:1 year 6 months')\n",
      "(u'Bear', u'Retriever, Chesapeake Bay', u'Mix', u'Age:2 years 5 months')\n",
      "(u'Teddy', u'Terrier, Jack Russell', u'Mix', u'Age:6 years 0 months')\n",
      "(u'Chloe', u'Akita', u'Mix', u'Age:5 years 0 months')\n",
      "(u'Drew', u'Retriever, Labrador', u'Retriever, Golden', u'Age:2 years 0 months')\n",
      "(u'Harley', u'Terrier, American Pit Bull', u'Mix', u'Age:2 years 0 months')\n",
      "(u'Sadie', u'Great Dane', u'Retriever, Labrador', u'Age:8 years 10 months')\n",
      "(u'Megrita', u'Dutch Shepherd', u'Mix', u'Age:5 years 0 months')\n",
      "(u'Molly', u'Mastiff', u'Rottweiler', u'Age:8 years 0 months')\n",
      "(u'Butterbean', u'Terrier, Jack Russell', u'Mix', u'Age:7 years 0 months')\n",
      "(u'Rowdy', u'Rottweiler', u'Mix', u'Age:6 years 0 months')\n",
      "(u'Marco', u'Spaniel, American Cocker', u'', u'Age:5 years 0 months')\n",
      "(u'Eva', u'Boxer', u'Mix', u'Age:6 years 0 months')\n",
      "(u'Stew', u'Chihuahua, Short Coat', u'Mix', u'Age:7 years 0 months')\n",
      "(u'Sandlin', u'German Shepherd', u'Mix', u'Age:0 years 5 months')\n",
      "(u'Gabby', u'Akita', u'Mix', u'Age:0 years 10 months')\n",
      "(u'Tom', u'Beagle', u'Mix', u'Age:10 years 0 months')\n",
      "(u'Jenny', u'Coonhound, Black and Tan', u'', u'Age:6 years 3 months')\n",
      "(u'Shadow', u'German Shepherd', u'Mix', u'Age:1 year 3 months')\n",
      "(u'Dolly', u'Chihuahua, Short Coat', u'Mix', u'Age:11 years 0 months')\n",
      "(u'Sandy May', u'Coonhound', u'Mix', u'Age:7 years 0 months')\n",
      "(u'Spoodles', u'Chihuahua, Long Coat', u'Poodle, Miniature', u'Age:5 years 6 months')\n",
      "(u'Bruno', u'Vizsla, Smooth Haired', u'Beagle', u'Age:1 year 6 months')\n",
      "(u'Fen', u'Pointer', u'Mix', u'Age:1 year 7 months')\n",
      "(u'Elijah', u'Terrier, American Pit Bull', u'Mix', u'Age:0 years 8 months')\n",
      "(u'Madre', u'Terrier, American Pit Bull', u'Mix', u'Age:3 years 1 month')\n",
      "(u'Lulu', u'Boxer', u'Mix', u'Age:4 years 6 months')\n",
      "(u'Baloo', u'Beagle', u'Mix', u'Age:0 years 8 months')\n",
      "(u'Rocky', u'Retriever, Labrador', u'Mix', u'Age:3 years 1 month')\n",
      "(u'Bailey', u'Schnauzer, Miniature', u'Mix', u'Age:3 years 1 month')\n",
      "(u'Napoleon', u'Boxer', u'Mix', u'Age:0 years 6 months')\n"
     ]
    }
   ],
   "source": [
    "head = \"views-field views-field-field-pp-\"\n",
    "for pet in pets:\n",
    "    name = pet.find('div', {'class': head + 'animalname'}).get_text(strip=True)\n",
    "    primary_breed = pet.find('div', {'class': head + 'primarybreed'}).get_text(strip=True)\n",
    "    secondary_breed = pet.find('div', {'class': head + 'secondarybreed'}).get_text(strip=True)\n",
    "    age = pet.find('div', {'class': head + 'age'}).get_text(strip=True)\n",
    "    print(name, primary_breed, secondary_breed, age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where each call to `find` is getting the children of a pet object, in particular, the `div`s with `class` attributes which look like `views-field views-field-field-pp-*`. Feel free to replace the above code with the cat or small animal pages provided and see how the output changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic (Javascript) Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the webpages we were loading required no [Javascript](https://en.wikipedia.org/wiki/JavaScript). In other words, there was no input required on the users end to view the content of the page (e.g. a login). Let's try a more complicated example of webscraping where content is loaded dynamically.\n",
    "\n",
    "\n",
    "Some characterirstics:\n",
    "- It's slow\n",
    "- It can handle javascript\n",
    "- You get **html** code back\n",
    "- Behave like a person\n",
    "\n",
    "\n",
    "Requirements (one):\n",
    "- Firefox + geckodriver (https://github.com/mozilla/geckodriver/releases)\n",
    "- Chrome + chromedriver\n",
    "\n",
    "    \n",
    "geckodriver/chromedriver must have execution permissions (chmod +x geckodriver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define two functions to wait until the page has finished loading. Most times this is not needed but it doesn't hurt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import selenium.webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the browser and define how much are you willing to wait for a page to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#open the driver (change the executable path to geckodriver_mac or geckodriver.exe)\n",
    "driver = selenium.webdriver.Firefox(executable_path=\"./geckodriver\")\n",
    "driver.implicitly_wait(10)\n",
    "driver.set_page_load_timeout(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get xkcd and click through the comics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a website\n",
    "driver.get(\"https://xkcd.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's find the random buttom\n",
    "element = driver.find_element_by_xpath('//*[@id=\"middleContainer\"]/ul[1]/li[3]/a')\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find an attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This happens in geek circles every so often. The 'Hey, this is just a system I can figure out easily!' is also a problem among engineers first diving into the stock market.\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element = driver.find_element_by_xpath('//*[@id=\"comic\"]/img')\n",
    "element.get_attribute(\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log in in spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##DO NOT WRITE YOUR PASSWORD IN NOTEBOOKS!!\n",
    "fb_email, fb_pass = \"f1692418@mvrht.com\",\"pedropalotes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#go to facebook\n",
    "driver.get(\"https://www.facebook.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#send email and password\n",
    "driver.find_element_by_xpath('//*[@id=\"email\"]').send_keys(fb_email)\n",
    "driver.find_element_by_xpath('//*[@id=\"pass\"]').send_keys(fb_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#click on login\n",
    "driver.find_element_by_xpath('//*[@id=\"loginbutton\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find JP\n",
    "element = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div/div[1]/div/div/div/div[1]/div[2]/div/form/div/div/div/div/input[2]')\n",
    "element.send_keys(\"john paul gonzales\")\n",
    "element = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div/div[1]/div/div/div/div[1]/div[2]/div/form/button')\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#click on him\n",
    "element = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[1]/div/div[3]/div[2]/div/div/div[3]/div/div/div/div[1]/div/div/div/div/div/div[2]/div[1]/div/div[1]/a/div')\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#send a friend request\n",
    "element = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[1]/div/div[2]/div[2]/div[2]/div/div[1]/div/div[4]/div/div[2]/div/div[2]/span/span/span[1]/a')\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow users to access large amounts of data, companies may provide an [Application Programming Interface (API)](https://en.wikipedia.org/wiki/Application_programming_interface). Often these request are handled via PUT and POST HTTP requests. For example, to make a request from the Twitter API:\n",
    "\n",
    "```{bash}\n",
    "curl --request GET \n",
    " --url 'https://api.twitter.com/1.1/search/tweets.json?q=nasa&result_type=popular' \n",
    " --header 'authorization: OAuth oauth_consumer_key=\"consumer-key-for-app\", ... , \n",
    " oauth_token=\"access-token-for-authed-user\", oauth_version=\"1.0\"'\n",
    " ```\n",
    "\n",
    "APIs often return data in the format of [Javascript Object Notation (JSON)](https://en.wikipedia.org/wiki/JSON). For example:\n",
    "\n",
    "```{json}\n",
    "{\"status\": 200, \"message\": \"hello world\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Hidden\" APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
